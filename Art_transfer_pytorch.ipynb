{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, models\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).features\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image_tensor):\n",
    "    image = image_tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    \n",
    "def save_image(image_tensor,i):\n",
    "    image = image_tensor.to(\"cpu\").clone().detach()\n",
    "    image = image.numpy().squeeze()\n",
    "    image = image.transpose(1,2,0)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    plt.imsave(\"/home/abdullah/Documents/ai/Style Transfer/results/{}.jpg\".format(i),image)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path):\n",
    "    image = Image.open(path).convert('RGB')\n",
    "    transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                            (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    image = transform(image)[:3,:,:].unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "     \n",
    "def get_features(image, model, layers=None):\n",
    "    \"\"\" Run an image forward through a model and get the features for \n",
    "        a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n",
    "    \"\"\"\n",
    "\n",
    "    # To get the layer outputs we need to pass the image forward through the network \n",
    "    # until we get to a desired layer and get the output from that layer\n",
    "    \n",
    "    # Mapping layer names of PyTorch's VGGNet to layer names from the paper\n",
    "    if layers is None:\n",
    "        layers = {'0': 'conv1_1',\n",
    "                  '5': 'conv2_1', \n",
    "                  '10': 'conv3_1', \n",
    "                  '19': 'conv4_1',\n",
    "                  '21': 'conv4_2',\n",
    "                  '28': 'conv5_1'}\n",
    "    features = {}\n",
    "    x = image\n",
    "    # model._modules is a dictionary holding each module in the model\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "            \n",
    "    return features\n",
    "\n",
    "def gram_mat(tensor):\n",
    "    b,c,h,w = tensor.size()\n",
    "    image = tensor.view(b*c,h*w)\n",
    "    \n",
    "    matrix = torch.mm(image,image.t())\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.cuda().eval()\n",
    "content_img = load_img(\"1.png\").cuda()\n",
    "style_img = load_img(\"2.jpg\").cuda()\n",
    "\n",
    "content_feature = get_features(content_img, vgg)\n",
    "style_feature  = get_features(style_img, vgg)\n",
    "\n",
    "style_gram = {label : gram_mat(style_feature[label]) for label in style_feature}\n",
    "\n",
    "target_img = content_img.clone().requires_grad_(True).cuda()\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "loss1 = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 th iteration\n",
      "2 th iteration\n",
      "3 th iteration\n",
      "4 th iteration\n",
      "5 th iteration\n",
      "6 th iteration\n",
      "7 th iteration\n",
      "8 th iteration\n"
     ]
    }
   ],
   "source": [
    "style_weights = {'conv1_1': 1.,\n",
    "                 'conv2_1': 0.75,\n",
    "                 'conv3_1': 0.2,\n",
    "                 'conv4_1': 0.2,\n",
    "                 'conv5_1': 0.2}\n",
    "\n",
    "optimizer = optim.Adam([target_img],lr=0.003)\n",
    "\n",
    "steps = 5000\n",
    "content_weight = 1\n",
    "style_weight = 1e6\n",
    "tv_weight = 1e-6\n",
    "show_every = 3\n",
    "\n",
    "for i in range(1,steps + 1):\n",
    "    style_loss = 0\n",
    "    target_features = get_features(target_img, vgg)\n",
    "    print(\"{} th iteration\".format(i))\n",
    "    content_loss = loss(target_features['conv4_2'],content_feature['conv4_2'])\n",
    "    \n",
    "    for layer in style_weights:\n",
    "        target_feature_ = target_features[layer]\n",
    "        target_gram  = gram_mat(target_feature_)\n",
    "        b, c, h, w = target_feature_.shape\n",
    "        style_loss += style_weights[layer] * torch.mean((target_gram - style_gram[layer])**2) / (c * h * w)\n",
    "    diff_i = torch.sum(torch.abs(target_img[:, :, :, 1:] - target_img[:, :, :, :-1]))\n",
    "    diff_j = torch.sum(torch.abs(target_img[:, :, 1:, :] - target_img[:, :, :-1, :]))\n",
    "    tv_loss = (diff_i + diff_j)\n",
    "    \n",
    "    total_loss = content_loss*content_weight + style_loss*style_weight + tv_weight*tv_loss\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (i%50 == 0):\n",
    "        save_image(target_img,i)\n",
    "        show_image(target_img)\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
